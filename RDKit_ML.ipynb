{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDKIT DESCRIPTORS. \n",
    "## COVID19 - ANTIVIRALS PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/adriana/Desktop/TFM/dataset/rdkit')\n",
    "import time\n",
    "import pyqsar\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Descriptors\n",
    "from pandas import read_csv\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "#Processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, StackingClassifier, RandomTreesEmbedding\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRdkitFeatures (descriptors, data):\n",
    "    '''Gives a table of features from a smile. Descriptor's list is given by the user.'''\n",
    "\n",
    "    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptors) # first it creates a calculator object with the descriptors\n",
    "    #print(calculator.GetDescriptorSummaries()) # if we want to get a calculator summary\n",
    "    values = [] # creates a list to append each value\n",
    "    \n",
    "    for smile in data.smiles:\n",
    "        mol = Chem.MolFromSmiles(smile) # it creates a rdkit molecule object for each smile data\n",
    "        if mol is None: continue\n",
    "        desc = calculator.CalcDescriptors(mol) # generates a tuple with the features for each smile\n",
    "        values.append(desc) # it accumulates each tuple in a list\n",
    "        features = pd.DataFrame(values, columns = descriptors) # transform the list into a dataframe\n",
    "    \n",
    "    return features\n",
    "\n",
    "def ProcessData(i):\n",
    "    '''Replace not float values with NaN.'''\n",
    "    try: \n",
    "        return float(i)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def GetColumns (data):\n",
    "    '''Gets those columns with a percentage of cells with NaN values >= 50%.'''\n",
    "    columns = []\n",
    "    for col in data:\n",
    "        percent = data[col].isnull().sum()/data[col].isnull().count()\n",
    "        if percent >= 0.5:\n",
    "            columns.append(col)\n",
    "    return columns\n",
    "\n",
    "def ML_score (models, X_train, Y_train, X_test, Y_test, seed, classes = ['0','1']):\n",
    "    '''Fit diferent models, predict and return models' scores'''\n",
    "    ACC = 0\n",
    "    AUROC = 0\n",
    "    precision = 0 \n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    \n",
    "    model_name = type(models).__name__ # get model name\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train different models using cross validation \n",
    "    print('> Training time: %0.2f mins'% ((time.time()-start_time)/60))\n",
    "        \n",
    "    models.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = models.predict(X_test)\n",
    "    y_probs = models.predict_proba(X_test)[:, 1]\n",
    "    model_report = classification_report(Y_test, y_pred, target_names=classes, output_dict=True, digits=3)\n",
    "     \n",
    "    # Scores\n",
    "    ACC = accuracy_score(Y_test, y_pred)\n",
    "    AUROC = roc_auc_score(Y_test, y_probs)\n",
    "    precision = model_report['weighted avg']['precision']\n",
    "    recall = model_report['weighted avg']['recall']\n",
    "    f1score = model_report['weighted avg']['f1-score']\n",
    "    \n",
    "    return ACC, AUROC, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIPTORS CALCULATION\n",
    "To create a predictive model, first we need to extract as many descriptors as we can from the smiles. \n",
    "In this case we had used Rdkit libraries which extracts 200 descriptors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "antiv = read_csv('antivirals_SMILES.csv')\n",
    "drugs = read_csv('DB_SMILES4prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RDKIT descriptors from smiles\n",
    "des_list = [x[0] for x in Descriptors._descList] # get all rdkit posible descriptors\n",
    "\n",
    "antiv_rdkit = pd.concat([antiv, GetRdkitFeatures(des_list, antiv)], axis=1)\n",
    "drugs_rdkit = pd.concat([drugs, GetRdkitFeatures(des_list, drugs)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save descriptors \n",
    "antiv_rdkit.to_csv('antiv_rdkit.csv', index_label=False)\n",
    "drugs_rdkit.to_csv('drugs_rdkit.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET PREPROCESSING\n",
    "Visualize if our datasets have smiles with NaN values and remove them. Transform every non-float character into NaN values.\n",
    "Get every descriptor that has more than 50% of NaN values and remove them. Other NaN values into 0 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature datasets\n",
    "antiv_rdkit = read_csv('antiv_rdkit.csv', low_memory=False) #to solve different column types\n",
    "antiv = antiv_rdkit.copy() # train\n",
    "\n",
    "drugs_rdkit = read_csv('drugs_rdkit.csv', low_memory=False) #to solve different column types\n",
    "drugs = drugs_rdkit.copy() # predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ids and features\n",
    "a = antiv.iloc[:,2:]\n",
    "id_a = antiv.loc[:,:'Class']\n",
    "\n",
    "d = drugs.iloc[:,3:]\n",
    "id_d = drugs.loc[:,:'Class']\n",
    "\n",
    "# Replace different column types with NaN values\n",
    "a = a.applymap(ProcessData)\n",
    "d = d.applymap(ProcessData)\n",
    "\n",
    "# Restore datasets\n",
    "antiv = pd.concat([id_a, a], axis=1)\n",
    "drugs = pd.concat([id_d, d], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First aproximation: any NaN value?\n",
    "print('Has Antivirals dataset NaN values?', antiv.isnull().values.any()) #true\n",
    "print('>> Columns with NaN: ', antiv.isnull().any().sum(), ' / ', len(antiv.columns))\n",
    "print('>> Number of data points with NaN: ', antiv.isnull().any(axis=1).sum(), ' / ', len(antiv))\n",
    "print('>> Number of rows with all NaN values: ', antiv.loc[:,'MaxEStateIndex':].isnull().all(axis=1).sum())\n",
    "\n",
    "print('\\nHas Drugs dataset NaN values?', drugs.isnull().values.any()) #true\n",
    "print('>> Columns with NaN: ', drugs.isnull().any().sum(), ' / ', len(drugs.columns))\n",
    "print('>> Number of data points with NaN: ', drugs.isnull().any(axis=1).sum(), ' / ', len(drugs))\n",
    "print('>> Number of rows with all NaN values: ', drugs.loc[:,'MaxEStateIndex':].isnull().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 8 rows with all NaN values in drugs dataset\n",
    "all_NA = [10246,10247,10248,10249,10250,10251,10252,10253] # all nan values from 10246 till 10253\n",
    "drugs = drugs.drop(all_NA) # remove 8 drugs from the drug dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop from antivirals dataset\n",
    "to_drop = GetColumns(antiv)\n",
    "\n",
    "# Drop the same columns in each dataset\n",
    "antiv.drop(to_drop, axis=1, inplace=True) #same columns are removed\n",
    "drugs.drop(to_drop, axis=1, inplace=True) #same columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second aproximation \n",
    "print('Has Antivirals NaN values?', antiv.isnull().values.any()) #false\n",
    "print('Has Drugs NaN values?', drugs.isnull().values.any()) #true\n",
    "\n",
    "# Replace any NaN value with 0\n",
    "antiv = antiv.fillna(0)\n",
    "drugs = drugs.fillna(0)\n",
    "\n",
    "print('Has Antivirals NaN values?', antiv.isnull().values.any()) #false\n",
    "print('Has Drugs NaN values?', drugs.isnull().values.any()) #false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed files\n",
    "antiv.to_csv('antiv_prepro_rdkit.csv', index_label=False)\n",
    "drugs.to_csv('drugs_prepro_rdkit.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION with antivirals dataset\n",
    "In this section we're going to select the descriptors. First we're going to separate our antivirals' dataset into train (80%) and test (20%) with a random_state of 80.\n",
    "Then we're going to Standardize them to get the same scale in each column. Lastly we're going to apply a feature selection method or a dimension reduction technique to delimit our dataset to maximum 50-100 descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "input_data = read_csv('antiv_prepro_rdkit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove smiles from dataset\n",
    "input_data = input_data.loc[:,'Class':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set categoricals\n",
    "input_data['Class'] = pd.Categorical(input_data['Class'])\n",
    "\n",
    "# Train and test dataset, one split 0.8 train, 0.2 test. Random_state=80\n",
    "x = input_data[input_data.iloc[:,1:].columns] \n",
    "y = input_data['Class']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2, random_state=80)\n",
    "\n",
    "print('Full dataset samples: {}'.format(input_data.shape[0]))\n",
    "print('Train dataset samples: {}'.format(x_train.shape[0]))\n",
    "print('Test dataset samples: {}'.format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data using only train set\n",
    "sc = StandardScaler().fit(x_train)\n",
    "sc.get_params();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "x_train_std.mean(axis=0);\n",
    "x_train_std.std(axis=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform splits from arrays into DataFrames\n",
    "df_train = pd.DataFrame(x_train_std, columns=list(input_data.iloc[:,1:].columns))\n",
    "df_train['Class'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame(x_test_std, columns=list(input_data.iloc[:,1:].columns))\n",
    "df_test['Class'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION - THRESHOLD\n",
    "# Remove features with low variance - th=0.0001\n",
    "featFilter = VarianceThreshold(threshold=0.0001) \n",
    "\n",
    "X_high_variance_train = featFilter.fit_transform(x_train_std) # fit and transform train dataset\n",
    "\n",
    "selected_features = set(list(df_train.columns[featFilter.get_support(indices=True)])) # features with high variance\n",
    "\n",
    "print('Features with high variance: {}'.format(len(featFilter.get_support(indices=True)))) # how many with high variance\n",
    "\n",
    "pool_features = set(list(df_train.columns)[:-1]) # all features\n",
    "print('Total number of features: ', len(pool_features))\n",
    "\n",
    "eliminated_feats = list(pool_features-selected_features)[:-1] # eliminated features\n",
    "print('Eliminated features: ', len(eliminated_feats))\n",
    "\n",
    "X_high_variance_test = featFilter.transform(x_test_std) # transform test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION - KBEST\n",
    "# Selection of the K Best features - mutual information k=70\n",
    "kbest = SelectKBest(mutual_info_classif, k=70)\n",
    "\n",
    "X_kbest_train = kbest.fit_transform(X_high_variance_train, y_train)\n",
    "\n",
    "print('Train dataset dimensions: ', X_kbest_train.shape) # 70 features\n",
    "\n",
    "X_kbest_test = kbest.transform(X_high_variance_test)\n",
    "\n",
    "print('Test dataset dimensions: ', X_kbest_test.shape) # 70 features\n",
    "\n",
    "features = set(list(df_train.columns[kbest.get_support(indices=True)])) # features with high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct files\n",
    "df_train = pd.DataFrame(X_kbest_train, columns=features)\n",
    "df_train['Class'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame(X_kbest_test, columns=features)\n",
    "df_test['Class'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "df_train.to_csv('train_rdkit.csv', index_label=False)\n",
    "df_test.to_csv('test_rdkit.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING - PREDICTIVE MODELS\n",
    "#### ML with train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = read_csv('train_rdkit.csv')\n",
    "test = read_csv('test_rdkit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 'Class'\n",
    "seed = 42 \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train values\n",
    "X_train = train.drop(out, axis=1).values\n",
    "Y_train = train[out].values\n",
    "\n",
    "# Test values\n",
    "X_test = test.drop(out, axis=1).values\n",
    "Y_test = test[out].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested models (baseline)\n",
    "models =  [LogisticRegression(random_state=seed, n_jobs=-1),\n",
    "          LinearDiscriminantAnalysis(),\n",
    "          QuadraticDiscriminantAnalysis(),\n",
    "          \n",
    "          DecisionTreeClassifier(random_state=seed),\n",
    "                     \n",
    "          SGDClassifier(loss='log',random_state=seed, n_jobs=-1),\n",
    "          NuSVC(random_state=seed, probability=True),\n",
    "          SVC(random_state=seed, probability=True),\n",
    "          \n",
    "          KNeighborsClassifier(n_jobs=-1),\n",
    "          GaussianProcessClassifier(random_state=seed, n_jobs=-1),\n",
    "          GaussianNB(),\n",
    "          \n",
    "          GradientBoostingClassifier(),\n",
    "          BaggingClassifier(random_state=seed),\n",
    "          AdaBoostClassifier(random_state=seed),\n",
    "          RandomForestClassifier(n_jobs=-1, random_state=seed),\n",
    "           \n",
    "          MLPClassifier(random_state=seed),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for ML scores\n",
    "df_ML = pd.DataFrame(columns=['Method', 'ACC', 'AUROC', 'Precision', 'Recall', 'F1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit each model\n",
    "for model in models:\n",
    "    print(\"\\n***\", model)\n",
    "    ACC, AUROC, precision, recall, f1score = ML_score(model, X_train, Y_train, X_test, Y_test, seed)\n",
    "    df_ML = df_ML.append({'Method': str(type(model).__name__),\n",
    "                          'ACC': float(ACC),\n",
    "                          'AUROC': float(AUROC),\n",
    "                          'Precision': float(precision),\n",
    "                          'Recall': float(recall),\n",
    "                          'F1-score': float(f1score)}, ignore_index=True)\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df_ML.to_csv('Scores_rdkit.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET THE BEST MODEL\n",
    "- GridSearchCV\n",
    "- Manual selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRIDSEARCH SELECTION\n",
    "# GridSearchCV parameters\n",
    "params = {\n",
    "    'activation' : ['identity','logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [100, 150, 200],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "    'beta_1': [0.5, 0.7, 0.9],\n",
    "    'beta_2': [0.5, 0.7, 0.9],\n",
    "    'epsilon':[0.00001, 0.0001, 0.001]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch fit\n",
    "gs = GridSearchCV(estimator=mlp, param_grid=params, verbose=10, scoring ='roc_auc', cv=3)\n",
    "\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores with GridSearch\n",
    "ACC, AUROC, precision, recall, f1score = ML_score(gs.best_estimator_, X_train, Y_train, X_test, Y_test, seed)\n",
    "print('ACC: ', ACC)\n",
    "print('AUROC', AUROC)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('F1Score', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL SELECTION\n",
    "mlp_best = MLPClassifier(random_state=seed, hidden_layer_sizes=49, beta_1=0.2, epsilon=0.00001)\n",
    "\n",
    "ACC, AUROC, precision, recall, f1score = ML_score(mlp_best, X_train, Y_train, X_test, Y_test, seed)\n",
    "print('ACC: ', ACC)\n",
    "print('AUROC', AUROC)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('F1Score', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "model_rdkit = 'bestmodel_rdkit.sav'\n",
    "pickle.dump(mlp_best, open(model_rdkit, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our predictions dataset with the same transformations: sc, featFilter and kbest\n",
    "# Load dataset with drugs to predict\n",
    "pred_data = read_csv('drugs_prepro_rdkit.csv')\n",
    "ids = pred_data.loc[:,'chembl_id']\n",
    "pred_data = pred_data.loc[:,'Class':] \n",
    "pred_data['Class'] = pd.Categorical(pred_data['Class'])\n",
    "\n",
    "# Remove inf values in pred_data\n",
    "pred_data.info()\n",
    "inf=pred_data.iloc[pred_data.values==np.inf]\n",
    "pred_data = pred_data.drop(36)\n",
    "\n",
    "# Separate values from unknown class\n",
    "pred_values = pred_data[pred_data.iloc[:,1:].columns]\n",
    "pred_class = pred_data['Class']\n",
    "\n",
    "#Standardize predictions values\n",
    "pred_std = sc.transform(pred_values.values)\n",
    "\n",
    "# Remove same antivirals features\n",
    "pred_highVar=featFilter.transform(pred_std) #low variance features\n",
    "pred_kbest=kbest.transform(pred_highVar) #70 best features\n",
    "\n",
    "# Reconstruct file\n",
    "df_pred = pd.DataFrame(pred_kbest, columns=features)\n",
    "\n",
    "# Save file\n",
    "df_pred['chembl_id'] = ids\n",
    "firstcol = df_pred.pop('chembl_id')\n",
    "df_pred.insert(0, 'chembl_id', firstcol)\n",
    "df_pred.to_csv('topredict.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read drugs dataset\n",
    "data = read_csv('topredict.csv')\n",
    "\n",
    "data_ids = data.loc[:,'chembl_id']\n",
    "data_feat = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our best model\n",
    "model_mlp = pickle.load(open('bestmodel_rdkit.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predict_mlp = model_mlp.predict(data_feat)\n",
    "prob_predict_mlp=model_mlp.predict_proba(data_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "dataframe_mlp = pd.DataFrame(columns=['smiles', 'ProbClass0', 'ProbClass1', 'Class'])\n",
    "dataframe_mlp['smiles']=list(data['chembl_id'])\n",
    "dataframe_mlp['ProbClass0']=list(prob_predict_mlp[:,0])\n",
    "dataframe_mlp['ProbClass1']=list(prob_predict_mlp[:,1])\n",
    "dataframe_mlp['Class']=list(predict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by class 1\n",
    "sort_mlp = dataframe_mlp.sort_values(by='ProbClass1', ascending=False)\n",
    "sort_mlp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top 20 predictions\n",
    "sort.to_csv('predictions_mlp.csv')\n",
    "top_20_mlp= sort_mlp.head(20)\n",
    "top_20_mlp.to_csv('top20_rdkit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC curve\n",
    "fig = plt.figure()\n",
    "noskill = plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill', color='lightblue')\n",
    "ax = plt.gca()\n",
    "metrics.plot_roc_curve(model_mlp, X_test, Y_test, ax=ax, alpha=0.8, color='blue')\n",
    "plt.title('AUROC curve MLPClassifier')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure\n",
    "fig.savefig('mlp_plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
